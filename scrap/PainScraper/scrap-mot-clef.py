from pyautogui import sleep
import requests
from bs4 import BeautifulSoup
from pyfiglet import Figlet
from colorama import Fore, Style
import time
from collections import Counter
import re
# Obj : 
# tester de scraper reddit pour trouver des problÃ¨mes dans une niche donnÃ©e via des mots-clÃ©s de "douleur", afin de rÃ©soudre cette problÃ©matique avec un futur tool, (SaaS).
def search_reddit_api(niche, subreddit="web_design", limit=10):
    """Solution de secours avec l'API Reddit"""
    try:
        url = f"https://www.reddit.com/r/{subreddit}/search.json?q={niche}&restrict_sr=1&sort=relevance&limit={limit}"
        headers = {'User-Agent': 'PainScraper/1.0 by berru-g'}
        
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            data = response.json()
            posts = []
            for post in data['data']['children'][:limit]:
                title = post['data']['title']
                selftext = post['data']['selftext']
                url = f"https://reddit.com{post['data']['permalink']}"
                posts.append({"text": f"{title} {selftext}", "url": url})
            return posts
        return []
    except Exception as e:
        print(f"   âŒ API Error: {e}")
        return []

def reddit_pain_scraper():
    f = Figlet(font='slant')
    print(f.renderText('Pain Scraper'))
    g = Figlet(font='small')
    print(g.renderText('Reddit/IndieHackers'))
    
    print(Fore.LIGHTBLUE_EX + "Reddit r/frontend")
    print(Fore.LIGHTGREEN_EX + "Reddit r/web_design") 
    print(Fore.LIGHTYELLOW_EX + "Reddit r/threejs")
    print(Fore.LIGHTMAGENTA_EX + "Reddit r/graphic_design")
    print(Fore.LIGHTWHITE_EX + "Changer les subreddit dans le code si besoin")

    # Liste de mots-clÃ©s dÃ©tectant des problÃ¨mes - Version design/tech
    pain_keywords = [
        # ProblÃ¨mes et difficultÃ©s
        "problÃ¨me", "problÃ¨mes", "difficile", "difficultÃ©", "difficultÃ©s", 
        "galÃ¨re", "galÃ©rer", "compliquÃ©", "complexe", "pas facile",
        "impossible", "ne marche pas", "bug", "buggÃ©", "plantage",
        "erreur", "dysfonctionnement", "ne fonctionne pas", "crash",
        
        # Frustrations et Ã©motions nÃ©gatives
        "frustrant", "frustration", "Ã©nervant", "agaÃ§ant", "horrible",
        "nul", "nulle", "pourri", "bizarre", "Ã©trange",
        "penible", "chiant", "insupportable", "dÃ©sagrÃ©able",
        "dÃ©courageant", "dÃ©motivant", "fatiguant", "Ã©puisant",
        
        # Temps et productivitÃ©
        "perte de temps", "trop long", "long", "chronophage", 
        "fastidieux", "rÃ©pÃ©titif", "manuel", "manuellement",
        "inefficace", "lent", "lenteur", "ralentissement",
        "duplicate", "doublon", "refaire", "recommencer",
        
        # CoÃ»ts et argent
        "cher", "chÃ¨re", "trop cher", "coÃ»teux", "coÃ»teuse",
        "hors de prix", "abonnement", "facturation", "tarif",
        "gratuit", "payant", "trop paye", "argent", "coÃ»t",
        
        # Recherche d'alternatives
        "alternative", "remplacer", "changer", "autre solution",
        "meilleur", "meilleure", "mieux", "comparer",
        "Ã©quivalent", "similaire", "solutions", "options",
        
        # Questions et aide
        "comment", "pourquoi", "aide", "aidez", "help",
        "solution", "rÃ©soudre", "corriger", "rÃ©parer",
        "conseil", "avis", "recommandez", "suggestions",
        
        # Manques et limitations
        "manque", "il manque", "absence", "pas de", "sans",
        "limitation", "limitÃ©e", "restreint", "insuffisant",
        "incomplet", "basique", "simple", "trop simple",
        
        # Apprentissage et comprÃ©hension
        "comprendre", "comprends pas", "expliquer",
        "dÃ©butant", "nouveau", "nouvelle", "apprendre",
        "tutoriel", "guide", "formation", "documentation",
        
        # SpÃ©cifique design/tech
        "client", "clients", "rÃ©vision", "modification", "feedback",
        "deadline", "inspiration", "creative block", "idÃ©es",
        "software", "outil", "adobe", "figma", "sketch", "photoshop",
        "performance", "optimisation", "loading", "seo", "accessibility",
        "animation", "scroll", "3D", "three.js", "webgl", "canvas",
        "responsive", "mobile", "cross-browser", "compatibility",
        "plugin", "extension", "library", "framework",
        "budget", "prix", "tarif", "facturation", "contrat",
        "template", "copier", "original", "unique",
        
        # Expressions courantes de plainte
        "je ne sais pas", "je sais pas", "perdu", "bloquÃ©",
        "Ã§a marche pas", "fonctionne pas", "sos",
        "urgence", "important", "critique", "grave",
        "normal", "anormal", "logique", "illogique",
        
        # Satisfaction nÃ©gative
        "dÃ©Ã§u", "dÃ©Ã§ue", "dÃ©ception", "insatisfait", "insatisfaite",
        "regrette", "dÃ©commandÃ©", "annulÃ©", "abandonnÃ©",
        
        # Recherche active
        "quelqu'un", "qqun", "des gens", "personne",
        "qui", "oÃ¹", "quand", "combien", "quel",
        
        # IntÃ©gration et compatibilitÃ©
        "compatible", "intÃ©gration", "import", "export",
        "connecter", "lien", "synchronisation", "sync"
    ]

    while True:
        print("â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®")
        print("| Entrez la niche/mÃ©tier:     |")
        print("â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯")
        niche = input("").strip().lower()
        
        print("â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®")
        print("| Combien de posts analyser?  |")
        print("â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯")
        try:
            post_count = int(input(""))
        except:
            post_count = 10

        print(" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

        sites = [
            {
                "name": "Reddit r/web_design",
                "url": f"https://www.reddit.com/r/web_design/search/?q={niche}&restrict_sr=1&sort=relevance",
                "subreddit": "web_design",
                "color": Fore.LIGHTBLUE_EX,
            },
            {
                "name": "Reddit r/Frontend", 
                "url": f"https://www.reddit.com/r/Frontend/search/?q={niche}&restrict_sr=1&sort=relevance",
                "subreddit": "Frontend",
                "color": Fore.LIGHTGREEN_EX,
            },
            {
                "name": "Reddit r/threejs",
                "url": f"https://www.reddit.com/r/threejs/search/?q={niche}&restrict_sr=1&sort=relevance",
                "subreddit": "threejs",
                "color": Fore.LIGHTYELLOW_EX,
            },
            {
                "name": "Reddit r/graphic_design",
                "url": f"https://www.reddit.com/r/graphic_design/search/?q={niche}&restrict_sr=1&sort=relevance",
                "subreddit": "graphic_design",
                "color": Fore.LIGHTMAGENTA_EX,
            }
        ]

        all_problems = []
        
        for site in sites:
            print(site["color"] + f"\nğŸ” Analyse de {site['name']}..." + Style.RESET_ALL)
            
            try:
                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
                
                site_search = requests.get(site["url"], headers=headers, timeout=10)
                print(f"   ğŸ“¡ Statut HTTP: {site_search.status_code}")
                print(f"   ğŸ“ Taille rÃ©ponse: {len(site_search.text)} caractÃ¨res")
                
                if site_search.status_code != 200:
                    print(f"   âŒ Reddit bloque la requÃªte, utilisation de l'API...")
                    api_posts = search_reddit_api(niche, site["subreddit"], post_count)
                    for post_data in api_posts:
                        post_text = post_data["text"].lower()
                        found_pains = [kw for kw in pain_keywords if kw in post_text]
                        if found_pains:
                            all_problems.append({
                                'text': post_text,
                                'pains': found_pains,
                                'source': site['name'],
                                'url': post_data["url"]
                            })
                else:
                    site_soup = BeautifulSoup(site_search.text, "html.parser")
                    
                    # ESSAIE DIFFÃ‰RENTS SÃ‰LECTEURS (Reddit change souvent)
                    posts = (
                        site_soup.select("h3._eYtD2XCVieq6emjKBH3m") or  # Nouveau sÃ©lecteur
                        site_soup.select("a[data-click-id='body']") or    # Ancien sÃ©lecteur
                        site_soup.select("a.title") or                    # TrÃ¨s ancien
                        site_soup.select("shreddit-post") or              # TrÃ¨s rÃ©cent
                        site_soup.find_all('h3') or                       # Fallback large
                        []
                    )
                    
                    if not posts:
                        print(f"   âŒ Aucun post trouvÃ© avec le scraping, utilisation de l'API...")
                        api_posts = search_reddit_api(niche, site["subreddit"], post_count)
                        for post_data in api_posts:
                            post_text = post_data["text"].lower()
                            found_pains = [kw for kw in pain_keywords if kw in post_text]
                            if found_pains:
                                all_problems.append({
                                    'text': post_text,
                                    'pains': found_pains,
                                    'source': site['name'],
                                    'url': post_data["url"]
                                })
                    else:
                        print(f"   âœ… {len(posts)} posts trouvÃ©s via scraping")
                        
                        # Pour le scraping, on va aussi essayer de rÃ©cupÃ©rer les URLs
                        site_problems = []
                        for i, post in enumerate(posts[:post_count]):
                            post_text = getattr(post, 'text', str(post)).lower()
                            
                            # Essayer de trouver l'URL associÃ©e
                            post_url = "URL non disponible (scraping)"
                            parent = post.find_parent('a')
                            if parent and parent.get('href'):
                                href = parent.get('href')
                                if href.startswith('/'):
                                    post_url = f"https://reddit.com{href}"
                                elif 'reddit.com' in href:
                                    post_url = href
                            
                            # DÃ©tection des mots de douleur
                            found_pains = []
                            for keyword in pain_keywords:
                                if keyword in post_text:
                                    found_pains.append(keyword)
                            
                            if found_pains:
                                problem_data = {
                                    'text': post_text,
                                    'pains': found_pains,
                                    'source': site['name'],
                                    'url': post_url
                                }
                                site_problems.append(problem_data)
                                
                                print(site["color"] + f"ğŸš¨ ProblÃ¨me dÃ©tectÃ© ({len(found_pains)} douleurs):")
                                print(f"   \"{post_text[:100]}...\"")
                                print(f"   Douleurs: {', '.join(found_pains)}")
                                print(f"   ğŸ”— {post_url}" + Style.RESET_ALL)
                                print("   " + "â”€" * 50)
                        
                        all_problems.extend(site_problems)
                        print(site["color"] + f"âœ… {len(site_problems)} problÃ¨mes trouvÃ©s sur {site['name']}" + Style.RESET_ALL)
                
            except Exception as e:
                print(f"âŒ Erreur sur {site['name']}: {e}")
                # Fallback Ã  l'API en cas d'erreur
                try:
                    print(f"   ğŸš‘ Tentative de sauvetage avec l'API...")
                    api_posts = search_reddit_api(niche, site["subreddit"], post_count)
                    for post_data in api_posts:
                        post_text = post_data["text"].lower()
                        found_pains = [kw for kw in pain_keywords if kw in post_text]
                        if found_pains:
                            all_problems.append({
                                'text': post_text,
                                'pains': found_pains,
                                'source': site['name'],
                                'url': post_data["url"]
                            })
                    print(site["color"] + f"âœ… {len([p for p in all_problems if p['source'] == site['name']])} problÃ¨mes trouvÃ©s via API" + Style.RESET_ALL)
                except Exception as api_error:
                    print(f"   âŒ Ã‰chec de l'API aussi: {api_error}")
            
            sleep(2)  # Pause pour Ã©viter le rate limiting

        # Analyse agrÃ©gÃ©e
        if all_problems:
            print(Fore.MAGENTA + "\n" + "â•" * 60)
            print("ğŸ“Š ANALYSE FINALE DES DOULEURS")
            print("â•" * 60)
            
            # Comptage des douleurs les plus frÃ©quentes
            pain_counter = Counter()
            source_counter = Counter()
            
            for problem in all_problems:
                pain_counter.update(problem['pains'])
                source_counter[problem['source']] += 1
            
            print("ğŸ¯ DOULEURS LES PLUS FRÃ‰QUENTES:")
            for pain, count in pain_counter.most_common(8):
                print(f"   â€¢ {pain}: {count} occurrences")
            
            print(f"\nğŸ“ˆ RÃ‰PARTITION PAR SOURCE:")
            for source, count in source_counter.most_common():
                print(f"   â€¢ {source}: {count} problÃ¨mes")
            
            print(f"\nğŸ”— LIENS VERS LES PROBLÃˆMES:")
            for i, problem in enumerate(all_problems[:10], 1):  # Affiche les 10 premiers
                print(f"   {i}. {problem['source']}")
                print(f"      {problem['url']}")
                if i == 10 and len(all_problems) > 10:
                    print(f"      ... et {len(all_problems) - 10} autres problÃ¨mes")
                    break
            
            print(f"\nğŸ“Š TOTAL: {len(all_problems)} problÃ¨mes identifiÃ©s")
            
            # Suggestions d'idÃ©es basÃ©es sur les douleurs
            top_pains = [pain for pain, count in pain_counter.most_common(3)]
            if top_pains:
                print(f"\nğŸ’¡ IDÃ‰ES POTENTIELLES:")
                for i, pain in enumerate(top_pains, 1):
                    print(f"   {i}. Outil pour rÃ©soudre: '{pain}'")
                    
        else:
            print(Fore.RED + "\nâŒ Aucun problÃ¨me dÃ©tectÃ© dans cette niche." + Style.RESET_ALL)
            print("ğŸ’¡ Suggestions:")
            print("   â€¢ Essayez d'autres mots-clÃ©s (ex: 'animation', 'scroll', '3D performance')")
            print("   â€¢ VÃ©rifiez votre connexion internet")
            print("   â€¢ Les subreddits peuvent Ãªtre trop spÃ©cifiques")

        print(Fore.CYAN + "\n" + "?" * 50)
        user_input = input("Nouvelle recherche ? (Oui/Non): ")
        if user_input.lower() != 'oui':
            print("ğŸ‘‹ Bonne chance pour trouver les prochaines idÃ©es!")
            break

if __name__ == "__main__":
    reddit_pain_scraper()